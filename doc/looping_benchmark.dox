namespace Cnes {
/**

\addtogroup looping_benchmark


\section looping_benchmark-model Simplistic computer model


Here is a simplistic computer model.
If you already know clock cycles, pipelining and cache misses, please skip this section.

The CPU executes instructions on some input data to produce output data, e.g. `c = a + b`.
Data (and instructions) come from memory: disk, RAM, cache(s).
Processing a single instruction really consists in:
- Fetching data from memory,
- Decoding the instruction,
- Executing it,
- Reading or writing in registers,
- Writing output data to memory.

Each of the steps takes one CPU clock cycle.
A cycle is the unit of measuremet of time, a clock tick, e.g. from which the CPU frequency is derived.

Modern CPUs implement two models of parallelism -- instruction parallelism and data parallelism:
- _Single instruction, multiple data (SIMD)_, or _vectorization_,
  is a set of technologies (SSE, SSE2, SSE3, AVX, AVX2, AVX512...)
  to exectue the same instructions on several inputs at once.
  SIMD code is generated either by the compiler or by gurus!
- _Pipelining_ is the concept of starting to fetch instruction 2
  while instruction 1 is being decoded, and so on (see table below);
  this results in the CPU being processing several instructions per cycle.

<table class="fieldtable">
<tr><th>Instruction<th>Cycle 1<th>Cycle 2<th>Cycle 3<th>Cycle 4<th>Cycle 5
<tr><td>1<td>Fetch<td>Decode<td>Execute<td>Memory<td>Write
<tr><td>2<td><td>Fetch<td>Decode<td>Execute<td>Memory
<tr><td>3<td><td><td>Fetch<td>Decode<td>Execute
<tr><td>4<td><td><td><td>Fetch<td>Decode
</table>

Before reaching the CPU, data passes from RAM to _cache_.
Here is how a cache works:
- If the needed byte is in CPU cache, use it;
- Else, copy it from RAM to cache.

Actually, there are several levels of cache (L1, L2, L3);
the closer to the CPU, the smaller and faster.

Writing to cache is _extremely_ slow, of the order of hundreds of cycles.
Using pipelining, a cache write can be up to a thousand times slower than an instruction.
That is a _cache miss_.
To mitigate this, caches are organized in lines, which are consecutive bytes.
When you need byte 42, the CPU caches bytes 43 to 105, too, just in case you'd need them later.
Then, accessing byte 43 is not a cache miss anymore.
This is why ensuring contiguity of pixels is of paramount importance.

Here is an overly simplified example to get orders of magnitude:
- `c = a + b;`
  - 1 instruction = 1 clock cycle (could be several instructions but you get the idea)
  - 3 cache misses = 600 clock cycles
  - = 602 clock cycles
- `for (int i = 0; i<64; ++i) { c[i] = a[i] + b[i]; }`
  - Accessing c[0] loads c[0] to c[63] in memory (one cache line)
  - 64 vectorized instructions = 8 clock cycles
  - 3 cache misses = 600 clock cycles
  - = 608 clock cycles <1% slower

Conclusion: The more instructions per cache miss, the better.

One of the key components for benefitting from parallelisms is the compiler,
which does an enormous amount of transforms to produce optimized instructions from code, like:
- Auto-inilining to avoid short functions = less sub-functions = better instruction locality;
- Intermediate variable generation = less recomputations + less memory accesses;
- Subscripting bypass = convert `operator[]()` into iterators;
- Vectorization = unroll loops to create SIMD instructions;
- Devirtualization of functions = bypass vtable of overriding function...

Here are a few intermediate take home messages:
- Cache misses are unavoidable -- each input data must be loaded at least once
- Cache misses are amazingly expensive -- 1 cache miss ~ 1000 instructions
- High-level optimization = minimize operations (algorithm, saves orders of magnitude)
- Mid-level optimization = minimize instructions per operation (implementation, saves factors)
- Low-level optimization = minimize cache misses per instruction (saves percents)
  - E.g. organize memory for expected computation
  - E.g. do everything you can with data already in cache


\section looping_benchmark-test_cases Benchmark test cases


The benchmark consists in computing the pixel-wise sum `m_c` of two 3D rasters `m_a` and `m_b`
of shape `{m_width, m_height, m_depth}`.
The following test cases are implemented and will be compared hereafter:

\snippet IterationBenchmark.cpp x-y-z
\snippet IterationBenchmark.cpp z-y-x
\snippet IterationBenchmark.cpp position
\snippet IterationBenchmark.cpp index
\snippet IterationBenchmark.cpp value
\snippet IterationBenchmark.cpp generate
\snippet IterationBenchmark.cpp operator


\section looping_benchmark-results Benchmark results


The shapes of the rasters are set to 400Â³ and the value type to `char`.
The benchmark consists in running 10 times each test case and comparing the minimum walltimes.
Absolute values are not relevant, only the relative deviations will be analyzed.

\image html doc/diagrams/looping_benchmark_crtp.svg

As ploted above, there are very significant discrepancies,
with one striking difference between the "x-y-z" and "z-y-x" cases.
Although they perform _exactly_ the same instructions on the same data, order is different.
In the first case, steps in the innermost loop are not contiguous in memory,
which results in many cache misses.
In the second case, the pixels are visited perfectly in order,
which minimizes cache misses.
There is no basic use case in which the "x-y-z" should be preferred.

Test case "position" is very close to "z-y-x" in the way it operates on the pixels:
they are visited in order, and the approach can be extended to rectangle regions easily.
Where "position" shines is when dimension is very high (having many nested loops should generally be avoided)
or when dimension is not even known at compile-time, in which case the approach of "z-y-x" is not applicable.
By contrast, "z-y-x" is easily parallelized
while "position" parallelization would require a manual partition of the iteration region.

The remaining test cases have in common the drawback that the knowledge of the position is lost
(there is no cheap way to get the position from the index).
While this is not relevant in the benchmark use case,
there might be situations in which the position is used in the evaluation of each pixel value.

Looping over indices is as efficient in terms of cache hits as the "z-y-x" and "position" cases.
Yet, iteration is much cheaper because the index is not computed from some formula at each iteration step:
it is simply incremeted.
Still, there are better approaches.

The value iterator is a fast option, and one which generalizes to more complex case, e.g. with strided iterators.
It could even be extended to region-wise loops.
This comes at the cost of verbosity and loss of clarity in the code.
Something which can be avoided through the use of `Raster::generate()`.
Indeed, "generate" compiles exactly the same way as "value",
thanks to the use of template metaprogramming under the hood.

TODO "operator"

<table class="fieldtable">
<tr><th>Test case<th>Run-time dimension<th>Region-wise iteration<th>Access to position<th>Character count<th>Use when...
<tr><th>x-y-z<td>No<td>Yes<td>Yes<td>127<td>Never! (Reverse loops order.)
<tr><th>z-y-x<td>No<td>Yes<td>Yes<td>127<td>Looping over regions of dimension known at compile-time.
<tr><th>position<td>Yes<td>Yes<td>Yes<td>52<td>Looping over regions of dimension known at run-time.
<tr><th>index<td>Yes<td>No<td>No<td>78<td>Never! (Prefer value iterator or operators.)
<tr><th>value<td>Yes<td>No<td>No<td>152<td>`Raster::generate()` does not fit.
<tr><th>generate<td>Yes<td>No<td>No<td>50<td>Whenever possible.
<tr><th>operator<td>Yes<td>No<td>No<td>12<td>Speed is less important than readability.
</table>

TODO conclusion

*/
}